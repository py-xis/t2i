{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffusers.training_utils import set_seed\n",
    "from src.prepare_easy import prepare_prompts_wiki398\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch._inductor.config.conv_1x1_as_mm = True\n",
    "torch._inductor.config.coordinate_descent_tuning = True\n",
    "torch._inductor.config.epilogue_fusion = False\n",
    "torch._inductor.config.coordinate_descent_check_all_directions = True\n",
    "\n",
    "import random\n",
    "\n",
    "import clip\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from src.eval.basic_metrics import calculate_mse, calculate_psnr_from_mse\n",
    "from src.eval.clipscore import clip_metrics, extract_all_images\n",
    "from src.eval.ocr_eval import get_ocr_easyocr, get_text_easyocr, ocr_metrics\n",
    "from src.eval.text_distance import get_levenshtein_distances\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.size\": 15,  # Set font size to 11pt\n",
    "        \"axes.labelsize\": 15,  # -> axis labels\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"lines.linewidth\": 2,\n",
    "        \"text.usetex\": False,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDXL_MODEL_NAME_OR_PATH = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "SEED = 42\n",
    "N_SAMPLES_PER_PROMPT = 1\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = \"cuda\"\n",
    "NUM_INFERENCE_STEPS = 50\n",
    "ATTENTIONS_TO_PATCH = [\n",
    "    55,\n",
    "    56,\n",
    "]  # NOTE: During patching we cache all activations thus it consumes a lot of memory. Try to patch only limited number of attentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].shape[1], imgs[0].shape[0]\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.astype(np.uint8)\n",
    "        grid.paste(Image.fromarray(img), box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    SDXL_MODEL_NAME_OR_PATH,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True if DEVICE == \"cuda\" else False,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    "    torch_dtype=torch.float32 if DEVICE == \"cpu\" else torch.float16,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = torch.randn(\n",
    "    (N_SAMPLES_PER_PROMPT, 4, 128, 128),\n",
    "    generator=generator,\n",
    "    dtype=torch.float32 if DEVICE == \"cpu\" else torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts_A, prompts_B = prepare_prompts_wiki400(\"data/wiki400.txt\", limit=8)\n",
    "prompts_A, prompts_B = prepare_prompts_wiki398(\"data/wiki398.txt\", limit=1)\n",
    "# prompts_A, prompts_B = prepare_prompts_mario_bechmark(\n",
    "#     \"/storage2/bartosz/code/t2i2/data/MARIOEval/DrawBenchText/DrawBenchText.txt\"\n",
    "# )\n",
    "\n",
    "\n",
    "# prompts_A = [{\"prompt\": \"A sign that says 'hate'\", \"text\": \"hate\"}]\n",
    "# prompts_B = [{\"prompt\": \"A sign that says 'love'\", \"text\": \"love\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts_A), len(prompts_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_model = get_ocr_easyocr(use_cuda=(DEVICE == \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, transform = clip.load(\"ViT-B/32\", device=DEVICE, jit=False)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    prompts,\n",
    "    noise,\n",
    "    batch_size,\n",
    "    num_inference_steps,\n",
    "    generator,\n",
    "    device,\n",
    "    run_with_cache,\n",
    "    attn_idx_to_patch=None,\n",
    "    timestep_start_patching=None,\n",
    "):\n",
    "    all_images = np.zeros((len(prompts), 1024, 1024, 3), dtype=np.float32)\n",
    "    with tqdm(total=len(prompts)) as pbar:\n",
    "        for batch_num, batch_start in enumerate(range(0, len(prompts), batch_size)):\n",
    "            prompt = prompts[batch_start : batch_start + batch_size]\n",
    "            latent = noise.repeat(len(prompt), 1, 1, 1).to(device)\n",
    "            images = pipe(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                generator=generator,\n",
    "                latents=latent,\n",
    "                run_with_cache=run_with_cache,\n",
    "                attn_idx_to_patch=attn_idx_to_patch,\n",
    "                output_type=\"np\",\n",
    "                batch_num=batch_num,\n",
    "                timestep_start_patching=timestep_start_patching,\n",
    "            ).images\n",
    "            images = images * 255\n",
    "            all_images[batch_start : batch_start + batch_size] = images\n",
    "            pbar.update(len(prompt))\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    original_images_A,\n",
    "    original_images_A_feats,\n",
    "    images,\n",
    "    texts_A,\n",
    "    texts_B,\n",
    "    prompts_A,\n",
    "    prompts_B,\n",
    "    device,\n",
    "    batch_size,\n",
    "):\n",
    "    # calculate metrics per sample\n",
    "    # 1. MSE\n",
    "    mse = calculate_mse(original_images_A, images)\n",
    "    # 2.PSNR\n",
    "    psnr = calculate_psnr_from_mse(mse)\n",
    "    # 3. SSIM\n",
    "    ssim_val = ssim(\n",
    "        torch.from_numpy(original_images_A).permute((0, 3, 1, 2)),\n",
    "        torch.from_numpy(images).permute((0, 3, 1, 2)),\n",
    "        data_range=255,\n",
    "        size_average=False,\n",
    "    ).numpy()\n",
    "    # 4. OCR Acc/Prec/Rec\n",
    "    ocr_texts = [\n",
    "        get_text_easyocr(ocr_model, images[i].astype(np.uint8)).lower()\n",
    "        for i in range(images.shape[0])\n",
    "    ]\n",
    "    ocr_pr_A, ocr_rec_A, ocr_acc_A = ocr_metrics(ocr_texts, texts_A)\n",
    "    ocr_pr_B, ocr_rec_B, ocr_acc_B = ocr_metrics(ocr_texts, texts_B)\n",
    "    # 5. CLIPScore\n",
    "    image_sim, prompt_A_sim, prompt_B_sim = clip_metrics(\n",
    "        clip_model,\n",
    "        images,\n",
    "        original_images_A_feats,\n",
    "        device,\n",
    "        batch_size,\n",
    "        prompts_A,\n",
    "        prompts_B,\n",
    "    )\n",
    "    # 6. Levenshtein distance\n",
    "    leve_A = get_levenshtein_distances(ocr_texts, texts_A)\n",
    "    leve_B = get_levenshtein_distances(ocr_texts, texts_B)\n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"PSNR\": psnr,\n",
    "        \"SSIM\": ssim_val,\n",
    "        \"OCR_A_Prec\": ocr_pr_A,\n",
    "        \"OCR_A_Rec\": ocr_rec_A,\n",
    "        \"OCR_A_Acc\": ocr_acc_A,\n",
    "        \"OCR_B_Prec\": ocr_pr_B,\n",
    "        \"OCR_B_Rec\": ocr_rec_B,\n",
    "        \"OCR_B_Acc\": ocr_acc_B,\n",
    "        \"CLIPScore_image\": image_sim,\n",
    "        \"CLIPScore_prompt_A\": prompt_A_sim,\n",
    "        \"CLIPScore_prompt_B\": prompt_B_sim,\n",
    "        \"Levenshtein_A\": leve_A,\n",
    "        \"Levenshtein_B\": leve_B,\n",
    "        \"Prompts_A\": prompts_A,\n",
    "        \"Prompts_B\": prompts_B,\n",
    "        \"OCR_texts\": ocr_texts,\n",
    "        \"Texts_A\": texts_A,\n",
    "        \"Texts_B\": texts_B,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_A = sample(\n",
    "    [p[\"prompt\"] for p in prompts_A],\n",
    "    noises,\n",
    "    BATCH_SIZE,\n",
    "    NUM_INFERENCE_STEPS,\n",
    "    generator,\n",
    "    DEVICE,\n",
    "    run_with_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_A_feats = extract_all_images(\n",
    "    original_images_A, clip_model, DEVICE, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_B = sample(\n",
    "    [p[\"prompt\"] for p in prompts_B],\n",
    "    noises,\n",
    "    BATCH_SIZE,\n",
    "    NUM_INFERENCE_STEPS,\n",
    "    generator,\n",
    "    DEVICE,\n",
    "    run_with_cache=True,\n",
    "    attn_idx_to_patch=ATTENTIONS_TO_PATCH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_A_metrics = calculate_metrics(\n",
    "    original_images_A,\n",
    "    original_images_A_feats,\n",
    "    original_images_A,\n",
    "    [p[\"text\"] for p in prompts_A],\n",
    "    [p[\"text\"] for p in prompts_B],\n",
    "    [p[\"prompt\"] for p in prompts_A],\n",
    "    [p[\"prompt\"] for p in prompts_B],\n",
    "    DEVICE,\n",
    "    BATCH_SIZE,\n",
    ")\n",
    "\n",
    "original_images_B_metrics = calculate_metrics(\n",
    "    original_images_A,\n",
    "    original_images_A_feats,\n",
    "    original_images_B,\n",
    "    [p[\"text\"] for p in prompts_A],\n",
    "    [p[\"text\"] for p in prompts_B],\n",
    "    [p[\"prompt\"] for p in prompts_A],\n",
    "    [p[\"prompt\"] for p in prompts_B],\n",
    "    DEVICE,\n",
    "    BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_A_df = pd.DataFrame(\n",
    "    original_images_A_metrics,\n",
    ")\n",
    "original_images_A_df[\"Block_patched\"] = [\"-\" for _ in range(len(prompts_A))]\n",
    "original_images_B_df = pd.DataFrame(\n",
    "    original_images_B_metrics,\n",
    "    index=[\"Model\" for _ in range(len(prompts_A))],\n",
    ")\n",
    "original_images_B_df[\"Block_patched\"] = [\"Model\" for _ in range(len(prompts_B))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_df = pd.concat([original_images_A_df, original_images_B_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestep_start in tqdm(list(range(10))):\n",
    "    patched_images = sample(\n",
    "        [p[\"prompt\"] for p in prompts_A],\n",
    "        noises,\n",
    "        BATCH_SIZE,\n",
    "        NUM_INFERENCE_STEPS,\n",
    "        generator,\n",
    "        DEVICE,\n",
    "        run_with_cache=False,\n",
    "        attn_idx_to_patch=ATTENTIONS_TO_PATCH,\n",
    "        timestep_start_patching=timestep_start,\n",
    "    )\n",
    "    patched_images_metrics = calculate_metrics(\n",
    "        original_images_A,\n",
    "        original_images_A_feats,\n",
    "        patched_images,\n",
    "        [p[\"text\"] for p in prompts_A],\n",
    "        [p[\"text\"] for p in prompts_B],\n",
    "        [p[\"prompt\"] for p in prompts_A],\n",
    "        [p[\"prompt\"] for p in prompts_B],\n",
    "        DEVICE,\n",
    "        BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    patched_images_df = pd.DataFrame(\n",
    "        patched_images_metrics,\n",
    "    )\n",
    "    patched_images_df[\"Block_patched\"] = [\n",
    "        f\"T{str(ATTENTIONS_TO_PATCH)}_timestep{timestep_start}\"\n",
    "        for _ in range(len(prompts_A))\n",
    "    ]\n",
    "    np.save(\n",
    "        f\"patched_images_timestep_{timestep_start}.npy\", patched_images.astype(np.uint8)\n",
    "    )\n",
    "\n",
    "    all_metrics_df = pd.concat([all_metrics_df, patched_images_df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
